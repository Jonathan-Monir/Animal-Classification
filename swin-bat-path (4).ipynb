{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T23:45:20.278791Z",
     "iopub.status.busy": "2024-12-22T23:45:20.278477Z",
     "iopub.status.idle": "2024-12-22T23:46:51.638599Z",
     "shell.execute_reply": "2024-12-22T23:46:51.637681Z",
     "shell.execute_reply.started": "2024-12-22T23:45:20.278757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from torchvision.transforms import ToTensor\n",
    "#from torchvision.datasets.mnist import MNIST\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "from torchvision.ops.misc import MLP, Permute\n",
    "from torchvision.ops.stochastic_depth import StochasticDepth\n",
    "from torchvision.transforms._presets import ImageClassification, InterpolationMode\n",
    "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.models._utils import handle_legacy_interface\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:46:51.641157Z",
     "iopub.status.busy": "2024-12-22T23:46:51.640508Z",
     "iopub.status.idle": "2024-12-22T23:46:51.650221Z",
     "shell.execute_reply": "2024-12-22T23:46:51.649476Z",
     "shell.execute_reply.started": "2024-12-22T23:46:51.641128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Define the custom dataset class\n",
    "class AnimalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(self.annotations.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define the transform for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 for Swin Transformer\n",
    "    transforms.ToTensor(),         # Convert PIL Image to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:46:51.651356Z",
     "iopub.status.busy": "2024-12-22T23:46:51.651109Z",
     "iopub.status.idle": "2024-12-22T23:47:03.291017Z",
     "shell.execute_reply": "2024-12-22T23:47:03.290190Z",
     "shell.execute_reply.started": "2024-12-22T23:46:51.651331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for Swin-V2 compatibility\n",
    "    transforms.ToTensor(),         # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ImageNet weights\n",
    "])\n",
    "\n",
    "# Dataset 1: 100extanimals\n",
    "data_dir_100ext = \"/kaggle/input/imagenet2\"\n",
    "ext_dataset = datasets.ImageFolder(root=data_dir_100ext, transform=transform)\n",
    "\n",
    "# Dataset 2: 200auganimals\n",
    "data_dir_200aug = \"/kaggle/input/200auganimals\"\n",
    "aug_dataset = datasets.ImageFolder(root=data_dir_200aug, transform=transform)\n",
    "\n",
    "# Dataset 3: 100imagenet1\n",
    "data_dir_imagenet1 = \"/kaggle/input/imagenet1\"\n",
    "imagenet1_dataset = datasets.ImageFolder(root=data_dir_imagenet1, transform=transform)\n",
    "\n",
    "# Dataset 4: 100imagenet1\n",
    "#data_dir_imagenet2 = \"/kaggle/input/imagenet2\"\n",
    "#imagenet2_dataset = datasets.ImageFolder(root=data_dir_imagenet2, transform=transform)\n",
    "\n",
    "# Dataset 5: 1000extanimals\n",
    "#data_dir_100ext = \"/kaggle/input/1000animals\"\n",
    "#ext_1000_dataset = datasets.ImageFolder(root=data_dir_100ext, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:47:03.293333Z",
     "iopub.status.busy": "2024-12-22T23:47:03.292894Z",
     "iopub.status.idle": "2024-12-22T23:47:03.297331Z",
     "shell.execute_reply": "2024-12-22T23:47:03.296388Z",
     "shell.execute_reply.started": "2024-12-22T23:47:03.293303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ext_dataset.__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:47:03.298863Z",
     "iopub.status.busy": "2024-12-22T23:47:03.298552Z",
     "iopub.status.idle": "2024-12-22T23:47:03.330664Z",
     "shell.execute_reply": "2024-12-22T23:47:03.330045Z",
     "shell.execute_reply.started": "2024-12-22T23:47:03.298825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Define the dataset class for validation\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, class_to_idx, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with image IDs and labels.\n",
    "            root_dir (str): Directory with all the images.\n",
    "            class_to_idx (dict): Mapping from class names to indices.\n",
    "            transform (callable, optional): Optional transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])  # ImageID\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.annotations.iloc[idx, 1]]  # Class\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Paths\n",
    "val_dir = \"/kaggle/input/animal-test/AnimalTrainData/AnimalTrainData\"\n",
    "csv_file = os.path.join(val_dir, \"train.csv\")  # Assuming train.csv is in the same directory\n",
    "\n",
    "# Ensure the class_to_idx matches the train mapping\n",
    "class_to_idx = {\n",
    "    'beaver': 0, 'butterfly': 1, 'cougar': 2, 'crab': 3, 'crayfish': 4,\n",
    "    'crocodile': 5, 'dolphin': 6, 'dragonfly': 7, 'elephant': 8, 'flamingo': 9,\n",
    "    'kangaroo': 10, 'leopard': 11, 'llama': 12, 'lobster': 13, 'octopus': 14,\n",
    "    'pigeon': 15, 'rhino': 16, 'scorpion': 17\n",
    "}\n",
    "\n",
    "# Define transforms for validation (same as training, usually, but without augmentations)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to fit model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
    "])\n",
    "\n",
    "# Create the validation dataset and DataLoader\n",
    "val_dataset = AnimalDataset(csv_file=csv_file, root_dir=val_dir, class_to_idx=class_to_idx, transform=val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)  # Adjust batch_size and workers as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:47:03.331944Z",
     "iopub.status.busy": "2024-12-22T23:47:03.331650Z",
     "iopub.status.idle": "2024-12-22T23:47:03.336230Z",
     "shell.execute_reply": "2024-12-22T23:47:03.335426Z",
     "shell.execute_reply.started": "2024-12-22T23:47:03.331915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(val_loader.__dict__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:47:03.337947Z",
     "iopub.status.busy": "2024-12-22T23:47:03.337669Z",
     "iopub.status.idle": "2024-12-22T23:47:03.426100Z",
     "shell.execute_reply": "2024-12-22T23:47:03.425370Z",
     "shell.execute_reply.started": "2024-12-22T23:47:03.337918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Swin\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"SwinTransformer\",\n",
    "    \"Swin_T_Weights\",\n",
    "    \"Swin_S_Weights\",\n",
    "    \"Swin_B_Weights\",\n",
    "    \"Swin_V2_T_Weights\",\n",
    "    \"Swin_V2_S_Weights\",\n",
    "    \"Swin_V2_B_Weights\",\n",
    "    \"swin_t\",\n",
    "    \"swin_s\",\n",
    "    \"swin_b\",\n",
    "    \"swin_v2_t\",\n",
    "    \"swin_v2_s\",\n",
    "    \"swin_v2_b\",\n",
    "]\n",
    "\n",
    "\n",
    "def _patch_merging_pad(x: torch.Tensor) -> torch.Tensor:\n",
    "    H, W, _ = x.shape[-3:]\n",
    "    x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "    x0 = x[..., 0::2, 0::2, :]  # ... H/2 W/2 C\n",
    "    x1 = x[..., 1::2, 0::2, :]  # ... H/2 W/2 C\n",
    "    x2 = x[..., 0::2, 1::2, :]  # ... H/2 W/2 C\n",
    "    x3 = x[..., 1::2, 1::2, :]  # ... H/2 W/2 C\n",
    "    x = torch.cat([x0, x1, x2, x3], -1)  # ... H/2 W/2 4*C\n",
    "    return x\n",
    "\n",
    "\n",
    "torch.fx.wrap(\"_patch_merging_pad\")\n",
    "\n",
    "\n",
    "def _get_relative_position_bias(\n",
    "    relative_position_bias_table: torch.Tensor, relative_position_index: torch.Tensor, window_size: List[int]\n",
    ") -> torch.Tensor:\n",
    "    N = window_size[0] * window_size[1]\n",
    "    relative_position_bias = relative_position_bias_table[relative_position_index]  # type: ignore[index]\n",
    "    relative_position_bias = relative_position_bias.view(N, N, -1)\n",
    "    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().unsqueeze(0)\n",
    "    return relative_position_bias\n",
    "\n",
    "\n",
    "torch.fx.wrap(\"_get_relative_position_bias\")\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"Patch Merging Layer.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, norm_layer: Callable[..., nn.Module] = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input tensor with expected layout of [..., H, W, C]\n",
    "        Returns:\n",
    "            Tensor with layout of [..., H/2, W/2, 2*C]\n",
    "        \"\"\"\n",
    "        x = _patch_merging_pad(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)  # ... H/2 W/2 2*C\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMergingV2(nn.Module):\n",
    "    \"\"\"Patch Merging Layer for Swin Transformer V2.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, norm_layer: Callable[..., nn.Module] = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(2 * dim)  # difference\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input tensor with expected layout of [..., H, W, C]\n",
    "        Returns:\n",
    "            Tensor with layout of [..., H/2, W/2, 2*C]\n",
    "        \"\"\"\n",
    "        x = _patch_merging_pad(x)\n",
    "        x = self.reduction(x)  # ... H/2 W/2 2*C\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def shifted_window_attention(\n",
    "    input: Tensor,\n",
    "    qkv_weight: Tensor,\n",
    "    proj_weight: Tensor,\n",
    "    relative_position_bias: Tensor,\n",
    "    window_size: List[int],\n",
    "    num_heads: int,\n",
    "    shift_size: List[int],\n",
    "    attention_dropout: float = 0.0,\n",
    "    dropout: float = 0.0,\n",
    "    qkv_bias: Optional[Tensor] = None,\n",
    "    proj_bias: Optional[Tensor] = None,\n",
    "    logit_scale: Optional[torch.Tensor] = None,\n",
    "    training: bool = True,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        input (Tensor[N, H, W, C]): The input tensor or 4-dimensions.\n",
    "        qkv_weight (Tensor[in_dim, out_dim]): The weight tensor of query, key, value.\n",
    "        proj_weight (Tensor[out_dim, out_dim]): The weight tensor of projection.\n",
    "        relative_position_bias (Tensor): The learned relative position bias added to attention.\n",
    "        window_size (List[int]): Window size.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        shift_size (List[int]): Shift size for shifted window attention.\n",
    "        attention_dropout (float): Dropout ratio of attention weight. Default: 0.0.\n",
    "        dropout (float): Dropout ratio of output. Default: 0.0.\n",
    "        qkv_bias (Tensor[out_dim], optional): The bias tensor of query, key, value. Default: None.\n",
    "        proj_bias (Tensor[out_dim], optional): The bias tensor of projection. Default: None.\n",
    "        logit_scale (Tensor[out_dim], optional): Logit scale of cosine attention for Swin Transformer V2. Default: None.\n",
    "        training (bool, optional): Training flag used by the dropout parameters. Default: True.\n",
    "    Returns:\n",
    "        Tensor[N, H, W, C]: The output tensor after shifted window attention.\n",
    "    \"\"\"\n",
    "    B, H, W, C = input.shape\n",
    "    # pad feature maps to multiples of window size\n",
    "    pad_r = (window_size[1] - W % window_size[1]) % window_size[1]\n",
    "    pad_b = (window_size[0] - H % window_size[0]) % window_size[0]\n",
    "    x = F.pad(input, (0, 0, 0, pad_r, 0, pad_b))\n",
    "    _, pad_H, pad_W, _ = x.shape\n",
    "\n",
    "    shift_size = shift_size.copy()\n",
    "    # If window size is larger than feature size, there is no need to shift window\n",
    "    if window_size[0] >= pad_H:\n",
    "        shift_size[0] = 0\n",
    "    if window_size[1] >= pad_W:\n",
    "        shift_size[1] = 0\n",
    "\n",
    "    # cyclic shift\n",
    "    if sum(shift_size) > 0:\n",
    "        x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "\n",
    "    # partition windows\n",
    "    num_windows = (pad_H // window_size[0]) * (pad_W // window_size[1])\n",
    "    x = x.view(B, pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1], C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B * num_windows, window_size[0] * window_size[1], C)  # B*nW, Ws*Ws, C\n",
    "\n",
    "    # multi-head attention\n",
    "    if logit_scale is not None and qkv_bias is not None:\n",
    "        qkv_bias = qkv_bias.clone()\n",
    "        length = qkv_bias.numel() // 3\n",
    "        qkv_bias[length : 2 * length].zero_()\n",
    "    qkv = F.linear(x, qkv_weight, qkv_bias)\n",
    "    qkv = qkv.reshape(x.size(0), x.size(1), 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "    if logit_scale is not None:\n",
    "        # cosine attention\n",
    "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)\n",
    "        logit_scale = torch.clamp(logit_scale, max=math.log(100.0)).exp()\n",
    "        attn = attn * logit_scale\n",
    "    else:\n",
    "        q = q * (C // num_heads) ** -0.5\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "    # add relative position bias\n",
    "    attn = attn + relative_position_bias\n",
    "\n",
    "    if sum(shift_size) > 0:\n",
    "        # generate attention mask\n",
    "        attn_mask = x.new_zeros((pad_H, pad_W))\n",
    "        h_slices = ((0, -window_size[0]), (-window_size[0], -shift_size[0]), (-shift_size[0], None))\n",
    "        w_slices = ((0, -window_size[1]), (-window_size[1], -shift_size[1]), (-shift_size[1], None))\n",
    "        count = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                attn_mask[h[0] : h[1], w[0] : w[1]] = count\n",
    "                count += 1\n",
    "        attn_mask = attn_mask.view(pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1])\n",
    "        attn_mask = attn_mask.permute(0, 2, 1, 3).reshape(num_windows, window_size[0] * window_size[1])\n",
    "        attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        attn = attn.view(x.size(0) // num_windows, num_windows, num_heads, x.size(1), x.size(1))\n",
    "        attn = attn + attn_mask.unsqueeze(1).unsqueeze(0)\n",
    "        attn = attn.view(-1, num_heads, x.size(1), x.size(1))\n",
    "\n",
    "    attn = F.softmax(attn, dim=-1)\n",
    "    attn = F.dropout(attn, p=attention_dropout, training=training)\n",
    "\n",
    "    x = attn.matmul(v).transpose(1, 2).reshape(x.size(0), x.size(1), C)\n",
    "    x = F.linear(x, proj_weight, proj_bias)\n",
    "    x = F.dropout(x, p=dropout, training=training)\n",
    "\n",
    "    # reverse windows\n",
    "    x = x.view(B, pad_H // window_size[0], pad_W // window_size[1], window_size[0], window_size[1], C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, pad_H, pad_W, C)\n",
    "\n",
    "    # reverse cyclic shift\n",
    "    if sum(shift_size) > 0:\n",
    "        x = torch.roll(x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "\n",
    "    # unpad features\n",
    "    x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "torch.fx.wrap(\"shifted_window_attention\")\n",
    "\n",
    "\n",
    "class ShiftedWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    See :func:`shifted_window_attention`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        window_size: List[int],\n",
    "        shift_size: List[int],\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = True,\n",
    "        proj_bias: bool = True,\n",
    "        attention_dropout: float = 0.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if len(window_size) != 2 or len(shift_size) != 2:\n",
    "            raise ValueError(\"window_size and shift_size must be of length 2\")\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim, bias=proj_bias)\n",
    "\n",
    "        self.define_relative_position_bias_table()\n",
    "        self.define_relative_position_index()\n",
    "\n",
    "    def define_relative_position_bias_table(self):\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), self.num_heads)\n",
    "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "\n",
    "    def define_relative_position_index(self):\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1).flatten()  # Wh*Ww*Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "    def get_relative_position_bias(self) -> torch.Tensor:\n",
    "        return _get_relative_position_bias(\n",
    "            self.relative_position_bias_table, self.relative_position_index, self.window_size  # type: ignore[arg-type]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor with layout of [B, H, W, C]\n",
    "        Returns:\n",
    "            Tensor with same layout as input, i.e. [B, H, W, C]\n",
    "        \"\"\"\n",
    "        relative_position_bias = self.get_relative_position_bias()\n",
    "        return shifted_window_attention(\n",
    "            x,\n",
    "            self.qkv.weight,\n",
    "            self.proj.weight,\n",
    "            relative_position_bias,\n",
    "            self.window_size,\n",
    "            self.num_heads,\n",
    "            shift_size=self.shift_size,\n",
    "            attention_dropout=self.attention_dropout,\n",
    "            dropout=self.dropout,\n",
    "            qkv_bias=self.qkv.bias,\n",
    "            proj_bias=self.proj.bias,\n",
    "            training=self.training,\n",
    "        )\n",
    "\n",
    "\n",
    "class ShiftedWindowAttentionV2(ShiftedWindowAttention):\n",
    "    \"\"\"\n",
    "    See :func:`shifted_window_attention_v2`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        window_size: List[int],\n",
    "        shift_size: List[int],\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = True,\n",
    "        proj_bias: bool = True,\n",
    "        attention_dropout: float = 0.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim,\n",
    "            window_size,\n",
    "            shift_size,\n",
    "            num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            proj_bias=proj_bias,\n",
    "            attention_dropout=attention_dropout,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n",
    "        # mlp to generate continuous relative position bias\n",
    "        self.cpb_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 512, bias=True), nn.ReLU(inplace=True), nn.Linear(512, num_heads, bias=False)\n",
    "        )\n",
    "        if qkv_bias:\n",
    "            length = self.qkv.bias.numel() // 3\n",
    "            self.qkv.bias[length : 2 * length].data.zero_()\n",
    "\n",
    "    def define_relative_position_bias_table(self):\n",
    "        # get relative_coords_table\n",
    "        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n",
    "        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n",
    "        relative_coords_table = torch.stack(torch.meshgrid([relative_coords_h, relative_coords_w], indexing=\"ij\"))\n",
    "        relative_coords_table = relative_coords_table.permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n",
    "\n",
    "        relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n",
    "        relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n",
    "\n",
    "        relative_coords_table *= 8  # normalize to -8, 8\n",
    "        relative_coords_table = (\n",
    "            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / 3.0\n",
    "        )\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n",
    "\n",
    "    def get_relative_position_bias(self) -> torch.Tensor:\n",
    "        relative_position_bias = _get_relative_position_bias(\n",
    "            self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads),\n",
    "            self.relative_position_index,  # type: ignore[arg-type]\n",
    "            self.window_size,\n",
    "        )\n",
    "        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n",
    "        return relative_position_bias\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor with layout of [B, H, W, C]\n",
    "        Returns:\n",
    "            Tensor with same layout as input, i.e. [B, H, W, C]\n",
    "        \"\"\"\n",
    "        relative_position_bias = self.get_relative_position_bias()\n",
    "        return shifted_window_attention(\n",
    "            x,\n",
    "            self.qkv.weight,\n",
    "            self.proj.weight,\n",
    "            relative_position_bias,\n",
    "            self.window_size,\n",
    "            self.num_heads,\n",
    "            shift_size=self.shift_size,\n",
    "            attention_dropout=self.attention_dropout,\n",
    "            dropout=self.dropout,\n",
    "            qkv_bias=self.qkv.bias,\n",
    "            proj_bias=self.proj.bias,\n",
    "            logit_scale=self.logit_scale,\n",
    "            training=self.training,\n",
    "        )\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (List[int]): Window size.\n",
    "        shift_size (List[int]): Shift size for shifted window attention.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.\n",
    "        dropout (float): Dropout rate. Default: 0.0.\n",
    "        attention_dropout (float): Attention dropout rate. Default: 0.0.\n",
    "        stochastic_depth_prob: (float): Stochastic depth rate. Default: 0.0.\n",
    "        norm_layer (nn.Module): Normalization layer.  Default: nn.LayerNorm.\n",
    "        attn_layer (nn.Module): Attention layer. Default: ShiftedWindowAttention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: List[int],\n",
    "        shift_size: List[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        stochastic_depth_prob: float = 0.0,\n",
    "        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n",
    "        attn_layer: Callable[..., nn.Module] = ShiftedWindowAttention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = attn_layer(\n",
    "            dim,\n",
    "            window_size,\n",
    "            shift_size,\n",
    "            num_heads,\n",
    "            attention_dropout=attention_dropout,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = MLP(dim, [int(dim * mlp_ratio), dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.mlp.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = x + self.stochastic_depth(self.attn(self.norm1(x)))\n",
    "        x = x + self.stochastic_depth(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlockV2(SwinTransformerBlock):\n",
    "    \"\"\"\n",
    "    Swin Transformer V2 Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (List[int]): Window size.\n",
    "        shift_size (List[int]): Shift size for shifted window attention.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.\n",
    "        dropout (float): Dropout rate. Default: 0.0.\n",
    "        attention_dropout (float): Attention dropout rate. Default: 0.0.\n",
    "        stochastic_depth_prob: (float): Stochastic depth rate. Default: 0.0.\n",
    "        norm_layer (nn.Module): Normalization layer.  Default: nn.LayerNorm.\n",
    "        attn_layer (nn.Module): Attention layer. Default: ShiftedWindowAttentionV2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: List[int],\n",
    "        shift_size: List[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        stochastic_depth_prob: float = 0.0,\n",
    "        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n",
    "        attn_layer: Callable[..., nn.Module] = ShiftedWindowAttentionV2,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim,\n",
    "            num_heads,\n",
    "            window_size,\n",
    "            shift_size,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth_prob=stochastic_depth_prob,\n",
    "            norm_layer=norm_layer,\n",
    "            attn_layer=attn_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # Here is the difference, we apply norm after the attention in V2.\n",
    "        # In V1 we applied norm before the attention.\n",
    "        x = x + self.stochastic_depth(self.norm1(self.attn(x)))\n",
    "        x = x + self.stochastic_depth(self.norm2(self.mlp(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Swin Transformer from the `\"Swin Transformer: Hierarchical Vision Transformer using\n",
    "    Shifted Windows\" <https://arxiv.org/abs/2103.14030>`_ paper.\n",
    "    Args:\n",
    "        patch_size (List[int]): Patch size.\n",
    "        embed_dim (int): Patch embedding dimension.\n",
    "        depths (List(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (List(int)): Number of attention heads in different layers.\n",
    "        window_size (List[int]): Window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.\n",
    "        dropout (float): Dropout rate. Default: 0.0.\n",
    "        attention_dropout (float): Attention dropout rate. Default: 0.0.\n",
    "        stochastic_depth_prob (float): Stochastic depth rate. Default: 0.1.\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000.\n",
    "        block (nn.Module, optional): SwinTransformer Block. Default: None.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None.\n",
    "        downsample_layer (nn.Module): Downsample layer (patch merging). Default: PatchMerging.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: List[int],\n",
    "        embed_dim: int,\n",
    "        depths: List[int],\n",
    "        num_heads: List[int],\n",
    "        window_size: List[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        stochastic_depth_prob: float = 0.1,\n",
    "        num_classes: int = 1000,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        block: Optional[Callable[..., nn.Module]] = None,\n",
    "        downsample_layer: Callable[..., nn.Module] = PatchMerging,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if block is None:\n",
    "            block = SwinTransformerBlock\n",
    "        if norm_layer is None:\n",
    "            norm_layer = partial(nn.LayerNorm, eps=1e-5)\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        # split image into non-overlapping patches\n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    3, embed_dim, kernel_size=(patch_size[0], patch_size[1]), stride=(patch_size[0], patch_size[1])\n",
    "                ),\n",
    "                Permute([0, 2, 3, 1]),\n",
    "                norm_layer(embed_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        total_stage_blocks = sum(depths)\n",
    "        stage_block_id = 0\n",
    "        # build SwinTransformer blocks\n",
    "        for i_stage in range(len(depths)):\n",
    "            stage: List[nn.Module] = []\n",
    "            dim = embed_dim * 2**i_stage\n",
    "            for i_layer in range(depths[i_stage]):\n",
    "                # adjust stochastic depth probability based on the depth of the stage block\n",
    "                sd_prob = stochastic_depth_prob * float(stage_block_id) / (total_stage_blocks - 1)\n",
    "                stage.append(\n",
    "                    block(\n",
    "                        dim,\n",
    "                        num_heads[i_stage],\n",
    "                        window_size=window_size,\n",
    "                        shift_size=[0 if i_layer % 2 == 0 else w // 2 for w in window_size],\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        dropout=dropout,\n",
    "                        attention_dropout=attention_dropout,\n",
    "                        stochastic_depth_prob=sd_prob,\n",
    "                        norm_layer=norm_layer,\n",
    "                    )\n",
    "                )\n",
    "                stage_block_id += 1\n",
    "            layers.append(nn.Sequential(*stage))\n",
    "            # add patch merging layer\n",
    "            if i_stage < (len(depths) - 1):\n",
    "                layers.append(downsample_layer(dim, norm_layer))\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        num_features = embed_dim * 2 ** (len(depths) - 1)\n",
    "        self.norm = norm_layer(num_features)\n",
    "        self.permute = Permute([0, 3, 1, 2])  # B H W C -> B C H W\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.permute(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _swin_transformer(\n",
    "    patch_size: List[int],\n",
    "    embed_dim: int,\n",
    "    depths: List[int],\n",
    "    num_heads: List[int],\n",
    "    window_size: List[int],\n",
    "    stochastic_depth_prob: float,\n",
    "    weights: Optional[WeightsEnum],\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> SwinTransformer:\n",
    "    if weights is not None:\n",
    "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = SwinTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        depths=depths,\n",
    "        num_heads=num_heads,\n",
    "        window_size=window_size,\n",
    "        stochastic_depth_prob=stochastic_depth_prob,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=True))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "_COMMON_META = {\n",
    "    \"categories\": _IMAGENET_CATEGORIES,\n",
    "}\n",
    "\n",
    "\n",
    "class Swin_T_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/swin_t-704ceda3.pth\",\n",
    "        transforms=partial(\n",
    "            ImageClassification, crop_size=224, resize_size=232, interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 28288354,\n",
    "            \"min_size\": (224, 224),\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 81.474,\n",
    "                    \"acc@5\": 95.776,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 4.491,\n",
    "            \"_file_size\": 108.19,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a similar training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class Swin_S_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/swin_s-5e29d889.pth\",\n",
    "        transforms=partial(\n",
    "            ImageClassification, crop_size=224, resize_size=246, interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 49606258,\n",
    "            \"min_size\": (224, 224),\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 83.196,\n",
    "                    \"acc@5\": 96.360,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 8.741,\n",
    "            \"_file_size\": 189.786,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a similar training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class Swin_B_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/swin_b-68c6b09e.pth\",\n",
    "        transforms=partial(\n",
    "            ImageClassification, crop_size=224, resize_size=238, interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 87768224,\n",
    "            \"min_size\": (224, 224),\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 83.582,\n",
    "                    \"acc@5\": 96.640,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 15.431,\n",
    "            \"_file_size\": 335.364,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a similar training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class Swin_V2_T_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/swin_v2_t-b137f0e2.pth\",\n",
    "        transforms=partial(\n",
    "            ImageClassification, crop_size=256, resize_size=260, interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 28351570,\n",
    "            \"min_size\": (256, 256),\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 82.072,\n",
    "                    \"acc@5\": 96.132,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 5.94,\n",
    "            \"_file_size\": 108.626,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a similar training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class Swin_V2_S_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/swin_v2_s-637d8ceb.pth\",\n",
    "        transforms=partial(\n",
    "            ImageClassification, crop_size=256, resize_size=260, interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 49737442,\n",
    "            \"min_size\": (256, 256),\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 83.712,\n",
    "                    \"acc@5\": 96.816,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 11.546,\n",
    "            \"_file_size\": 190.675,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a similar training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "class Swin_V2_B_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/swin_v2_b-781e5279.pth\",\n",
    "        transforms=partial(\n",
    "            ImageClassification, crop_size=256, resize_size=272, interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        meta={\n",
    "            **_COMMON_META,\n",
    "            \"num_params\": 87930848,\n",
    "            \"min_size\": (256, 256),\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 84.112,\n",
    "                    \"acc@5\": 96.864,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 20.325,\n",
    "            \"_file_size\": 336.372,\n",
    "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a similar training recipe.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Swin_T_Weights.IMAGENET1K_V1))\n",
    "def swin_t(*, weights: Optional[Swin_T_Weights] = None, progress: bool = True, **kwargs: Any) -> SwinTransformer:\n",
    "    \"\"\"\n",
    "    Constructs a swin_tiny architecture from\n",
    "    `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows <https://arxiv.org/abs/2103.14030>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Swin_T_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Swin_T_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.Swin_T_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Swin_T_Weights.verify(weights)\n",
    "\n",
    "    return _swin_transformer(\n",
    "        patch_size=[4, 4],\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=[7, 7],\n",
    "        stochastic_depth_prob=0.2,\n",
    "        weights=weights,\n",
    "        progress=progress,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Swin_S_Weights.IMAGENET1K_V1))\n",
    "def swin_s(*, weights: Optional[Swin_S_Weights] = None, progress: bool = True, **kwargs: Any) -> SwinTransformer:\n",
    "    \"\"\"\n",
    "    Constructs a swin_small architecture from\n",
    "    `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows <https://arxiv.org/abs/2103.14030>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Swin_S_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Swin_S_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.Swin_S_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Swin_S_Weights.verify(weights)\n",
    "\n",
    "    return _swin_transformer(\n",
    "        patch_size=[4, 4],\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=[7, 7],\n",
    "        stochastic_depth_prob=0.3,\n",
    "        weights=weights,\n",
    "        progress=progress,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Swin_B_Weights.IMAGENET1K_V1))\n",
    "def swin_b(*, weights: Optional[Swin_B_Weights] = None, progress: bool = True, **kwargs: Any) -> SwinTransformer:\n",
    "    \"\"\"\n",
    "    Constructs a swin_base architecture from\n",
    "    `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows <https://arxiv.org/abs/2103.14030>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Swin_B_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Swin_B_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.Swin_B_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Swin_B_Weights.verify(weights)\n",
    "\n",
    "    return _swin_transformer(\n",
    "        patch_size=[4, 4],\n",
    "        embed_dim=128,\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[4, 8, 16, 32],\n",
    "        window_size=[7, 7],\n",
    "        stochastic_depth_prob=0.5,\n",
    "        weights=weights,\n",
    "        progress=progress,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Swin_V2_T_Weights.IMAGENET1K_V1))\n",
    "def swin_v2_t(*, weights: Optional[Swin_V2_T_Weights] = None, progress: bool = True, **kwargs: Any) -> SwinTransformer:\n",
    "    \"\"\"\n",
    "    Constructs a swin_v2_tiny architecture from\n",
    "    `Swin Transformer V2: Scaling Up Capacity and Resolution <https://arxiv.org/abs/2111.09883>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Swin_V2_T_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Swin_V2_T_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.Swin_V2_T_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Swin_V2_T_Weights.verify(weights)\n",
    "\n",
    "    return _swin_transformer(\n",
    "        patch_size=[4, 4],\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=[8, 8],\n",
    "        stochastic_depth_prob=0.2,\n",
    "        weights=weights,\n",
    "        progress=progress,\n",
    "        block=SwinTransformerBlockV2,\n",
    "        downsample_layer=PatchMergingV2,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Swin_V2_S_Weights.IMAGENET1K_V1))\n",
    "def swin_v2_s(*, weights: Optional[Swin_V2_S_Weights] = None, progress: bool = True, **kwargs: Any) -> SwinTransformer:\n",
    "    \"\"\"\n",
    "    Constructs a swin_v2_small architecture from\n",
    "    `Swin Transformer V2: Scaling Up Capacity and Resolution <https://arxiv.org/abs/2111.09883>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Swin_V2_S_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Swin_V2_S_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.Swin_V2_S_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Swin_V2_S_Weights.verify(weights)\n",
    "\n",
    "    return _swin_transformer(\n",
    "        patch_size=[4, 4],\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=[8, 8],\n",
    "        stochastic_depth_prob=0.3,\n",
    "        weights=weights,\n",
    "        progress=progress,\n",
    "        block=SwinTransformerBlockV2,\n",
    "        downsample_layer=PatchMergingV2,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "@handle_legacy_interface(weights=(\"pretrained\", Swin_V2_B_Weights.IMAGENET1K_V1))\n",
    "def swin_v2_b(*, weights: Optional[Swin_V2_B_Weights] = None, progress: bool = True, **kwargs: Any) -> SwinTransformer:\n",
    "    \"\"\"\n",
    "    Constructs a swin_v2_base architecture from\n",
    "    `Swin Transformer V2: Scaling Up Capacity and Resolution <https://arxiv.org/abs/2111.09883>`_.\n",
    "\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Swin_V2_B_Weights`, optional): The\n",
    "            pretrained weights to use. See\n",
    "            :class:`~torchvision.models.Swin_V2_B_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_\n",
    "            for more details about this class.\n",
    "\n",
    "    .. autoclass:: torchvision.models.Swin_V2_B_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Swin_V2_B_Weights.verify(weights)\n",
    "\n",
    "    return _swin_transformer(\n",
    "        patch_size=[4, 4],\n",
    "        embed_dim=128,\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[4, 8, 16, 32],\n",
    "        window_size=[8, 8],\n",
    "        stochastic_depth_prob=0.5,\n",
    "        weights=weights,\n",
    "        progress=progress,\n",
    "        block=SwinTransformerBlockV2,\n",
    "        downsample_layer=PatchMergingV2,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:47:03.427269Z",
     "iopub.status.busy": "2024-12-22T23:47:03.427021Z",
     "iopub.status.idle": "2024-12-22T23:47:03.438479Z",
     "shell.execute_reply": "2024-12-22T23:47:03.437775Z",
     "shell.execute_reply.started": "2024-12-22T23:47:03.427243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, early_stopping_patience=3, save_model=False, final=False, ID=0, model_name=\"resnet\"):\n",
    "    # Ensure the 'models' directory exists\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nStart epoch {epoch + 1}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct_train / total_train * 100\n",
    "        \n",
    "        # Adjust the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {running_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total * 100\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        if save_model:\n",
    "            new_text = \"\"\n",
    "            if final:\n",
    "                new_text = f\"_final{ID}\"\n",
    "            model_path = os.path.join(\"models\", f\"{model_name}_epoch_{epoch + 1}_{int(train_accuracy)}_{int(val_accuracy)}%{new_text}.pth\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            best_model_path = os.path.join(\"models\", \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Model improved and saved to {best_model_path}!\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T23:47:03.439646Z",
     "iopub.status.busy": "2024-12-22T23:47:03.439378Z",
     "iopub.status.idle": "2024-12-23T00:08:52.007151Z",
     "shell.execute_reply": "2024-12-23T00:08:52.006230Z",
     "shell.execute_reply.started": "2024-12-22T23:47:03.439617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "model = swin_v2_b(weights=weights)\n",
    "\n",
    "num_classes = 18\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "torch.save(model.state_dict(), \"Main_model.pth\")\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "#image1\n",
    "temp_loader = DataLoader(dataset=imagenet1_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(ext_dataset, batch_size=8, shuffle=False)\n",
    "train_model(model, temp_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=2)\n",
    "\n",
    "#aug\n",
    "temp_loader = DataLoader(dataset=aug_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(ext_dataset, batch_size=8, shuffle=False)\n",
    "train_model(model, temp_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=2)\n",
    "\n",
    "\n",
    "\n",
    "#val\n",
    "train_model(model, val_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=4,save_model = True, final=True, ID=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary\n",
    "!pip install torchviz\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Batch size of 1, image size 224x224\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Visualize the model\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.render(\"efficient_architecture\", format=\"png\")  # Save the architecture as a PNG file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "def convert_folder_to_kaggle_dataset(source_folder, username, dataset_slug, description):\n",
    "    \"\"\"\n",
    "    Converts a folder into a Kaggle dataset.\n",
    "    \n",
    "    Args:\n",
    "        source_folder (str): Path to the folder containing dataset files.\n",
    "        username (str): Your Kaggle username.\n",
    "        dataset_slug (str): The slug for the new dataset.\n",
    "        description (str): Description of the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_folder):\n",
    "        raise FileNotFoundError(f\"The folder '{source_folder}' does not exist.\")\n",
    "\n",
    "    dataset_metadata = {\n",
    "        \"title\": dataset_slug.replace(\"-\", \" \").title(),\n",
    "        \"id\": f\"{username}/{dataset_slug}\",\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "        \"description\": description\n",
    "    }\n",
    "    \n",
    "    metadata_file = os.path.join(source_folder, \"dataset-metadata.json\")\n",
    "    \n",
    "    # Save metadata to a JSON file\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    \n",
    "    print(\"Metadata file created.\")\n",
    "    \n",
    "    # Run Kaggle API command to create the dataset\n",
    "    try:\n",
    "        command = [\n",
    "            \"kaggle\", \"datasets\", \"create\", \n",
    "            \"-p\", source_folder,\n",
    "            \"--dir-mode\", \"zip\"\n",
    "        ]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"Dataset successfully created!\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"Error creating dataset:\")\n",
    "            print(result.stderr)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Kaggle CLI is not installed or not found. Ensure the Kaggle API is set up correctly.\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('/root/.config/kaggle', exist_ok=True)\n",
    "\n",
    "downloaded_path = '/kaggle/input/kagglejson/kaggle.json'  # Adjust this path as needed'\n",
    "\n",
    "# Destination path\n",
    "destination_path = '/root/.config/kaggle/kaggle.json'\n",
    "\n",
    "shutil.copy(downloaded_path, destination_path)\n",
    "\n",
    "os.chmod(destination_path, 0o600)\n",
    "\n",
    "print(\"Kaggle JSON file successfully copied and permissions set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #image1\n",
    "    model.load_state_dict(torch.load(\"Main_model.pth\"))\n",
    "\n",
    "    temp_loader = DataLoader(dataset=imagenet1_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "    validation_loader = DataLoader(ext_dataset, batch_size=8, shuffle=False)\n",
    "    train_model(model, temp_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=1)\n",
    "    \n",
    "    #aug\n",
    "    temp_loader = DataLoader(dataset=aug_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "    validation_loader = DataLoader(ext_dataset, batch_size=8, shuffle=False)\n",
    "    train_model(model, temp_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=3, save_model = True, final=True, ID=12, model_name = \"vit32_aug\")\n",
    "    \n",
    "    #val\n",
    "    train_model(model, val_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=3,save_model = True, final=True, ID=22, model_name = \"vit32_val\")\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "convert_folder_to_kaggle_dataset(\n",
    "    source_folder='/kaggle/working/models', \n",
    "    username='jonathanmamduah',\n",
    "    dataset_slug='swinmodels1', \n",
    "    description='A collection of animal images'\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Script is ready. Modify the function call with your specific parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #image1\n",
    "    model.load_state_dict(torch.load(\"Main_model.pth\"))\n",
    "\n",
    "    temp_loader = DataLoader(dataset=imagenet1_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "    validation_loader = DataLoader(ext_dataset, batch_size=8, shuffle=False)\n",
    "    train_model(model, temp_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=3, save_model = True, final=True, ID=55, model_name = \"vit32_ext\")\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "convert_folder_to_kaggle_dataset(\n",
    "    source_folder='/kaggle/working/models', \n",
    "    username='jonathanmamduah',\n",
    "    dataset_slug='vitmodels2', \n",
    "    description='A collection of animal images'\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Script is ready. Modify the function call with your specific parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T00:17:27.238312Z",
     "iopub.status.busy": "2024-12-23T00:17:27.237952Z",
     "iopub.status.idle": "2024-12-23T00:17:49.266957Z",
     "shell.execute_reply": "2024-12-23T00:17:49.265987Z",
     "shell.execute_reply.started": "2024-12-23T00:17:27.238278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "#model = swin_v2_b(weights=weights)\n",
    "num_classes = 18\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Class labels mapping\n",
    "class_labels = [\n",
    "    \"beaver\", \"butterfly\", \"cougar\", \"crab\", \"crayfish\",\n",
    "    \"crocodile\", \"dolphin\", \"dragonfly\", \"elephant\", \"flamingo\",\n",
    "    \"kangaroo\", \"leopard\", \"llama\", \"lobster\", \"octopus\",\n",
    "    \"pigeon\", \"rhino\", \"scorpion\"\n",
    "]\n",
    "\n",
    "# After training the last model, compute classification report and confusion matrix\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "report = classification_report(all_labels, all_predictions, target_names=class_labels)\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T00:09:18.501695Z",
     "iopub.status.busy": "2024-12-23T00:09:18.501166Z",
     "iopub.status.idle": "2024-12-23T00:09:18.506120Z",
     "shell.execute_reply": "2024-12-23T00:09:18.505147Z",
     "shell.execute_reply.started": "2024-12-23T00:09:18.501644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"/kaggle/working/swin_epoch_2_95_83%.pth\"))\n",
    "#train_model(model, val_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=2,save_model = True, final=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T00:09:18.507889Z",
     "iopub.status.busy": "2024-12-23T00:09:18.507309Z",
     "iopub.status.idle": "2024-12-23T00:09:18.741293Z",
     "shell.execute_reply": "2024-12-23T00:09:18.740365Z",
     "shell.execute_reply.started": "2024-12-23T00:09:18.507849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"/kaggle/working/swin_epoch_3_99_86%.pth\"))\n",
    "#train_model(model, val_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=2,save_model = True, final=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T00:09:18.743100Z",
     "iopub.status.busy": "2024-12-23T00:09:18.742700Z",
     "iopub.status.idle": "2024-12-23T00:09:18.756215Z",
     "shell.execute_reply": "2024-12-23T00:09:18.755492Z",
     "shell.execute_reply.started": "2024-12-23T00:09:18.743060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"/kaggle/working/swin_epoch_2_95_83%.pth\"))\n",
    "#train_model(model, val_loader, validation_loader, criterion, optimizer, scheduler, device, num_epochs=2,save_model = True, final=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:56:41.658900Z",
     "iopub.status.busy": "2024-12-16T15:56:41.658482Z",
     "iopub.status.idle": "2024-12-16T15:58:13.965753Z",
     "shell.execute_reply": "2024-12-16T15:58:13.964649Z",
     "shell.execute_reply.started": "2024-12-16T15:56:41.658860Z"
    }
   },
   "source": [
    "# KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T11:32:51.631111Z",
     "iopub.status.busy": "2024-12-17T11:32:51.630731Z",
     "iopub.status.idle": "2024-12-17T11:32:51.646707Z",
     "shell.execute_reply": "2024-12-17T11:32:51.645761Z",
     "shell.execute_reply.started": "2024-12-17T11:32:51.631082Z"
    }
   },
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def train_model_kfold(model_class, weights, dataset, criterion, device, num_epochs=5, n_splits=5, early_stopping_patience=3):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    all_accuracies = []\n",
    "    best_avg_accuracy = 0.0\n",
    "    best_model_path = \"best_model.pth\"\n",
    "\n",
    "    # Extract data and labels for StratifiedKFold\n",
    "    inputs_all = [x[0] for x in dataset]\n",
    "    labels_all = [x[1] for x in dataset]\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(inputs_all, labels_all)):\n",
    "        print(f\"\\n--- Starting Fold {fold + 1}/{n_splits} ---\")\n",
    "        \n",
    "        # Reinitialize the model for each fold\n",
    "        model = model_class(weights=weights)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Optimizer and Scheduler\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        # Split data into train and validation sets for this fold\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=4)\n",
    "        val_loader_fold = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "        # Early stopping setup\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        fold_val_accuracies = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs} (Fold {fold + 1})\")\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            # Training loop\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_accuracy = correct_train / total_train * 100\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Training Loss: {running_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader_fold:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader_fold)\n",
    "            val_accuracy = correct / total * 100\n",
    "            fold_val_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "            # Save the model for this epoch\n",
    "            model_path = f\"model_fold_{fold + 1}_epoch_{epoch + 1}_{int(train_accuracy)}_{int(val_accuracy)}%.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                torch.save(model.state_dict(), f\"best_model_fold_{fold + 1}.pth\")\n",
    "                print(\"Model improved and saved!\")\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "        print(f\"\\nFold {fold + 1} Average Validation Accuracy: {avg_val_accuracy:.2f}%\")\n",
    "        all_accuracies.append(avg_val_accuracy)\n",
    "\n",
    "        # Save the best average accuracy model\n",
    "        if avg_val_accuracy > best_avg_accuracy:\n",
    "            best_avg_accuracy = avg_val_accuracy\n",
    "            best_model_path = f\"best_model_fold_{fold + 1}.pth\"\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f\"All Fold Accuracies: {all_accuracies}\")\n",
    "    print(f\"Best Average Validation Accuracy: {best_avg_accuracy:.2f}%\")\n",
    "    print(f\"Best Model Saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T11:35:35.081593Z",
     "iopub.status.busy": "2024-12-17T11:35:35.081230Z",
     "iopub.status.idle": "2024-12-17T11:36:20.043890Z",
     "shell.execute_reply": "2024-12-17T11:36:20.042622Z",
     "shell.execute_reply.started": "2024-12-17T11:35:35.081564Z"
    }
   },
   "source": [
    "# Define the model class and weights\n",
    "model_class = swin_v2_b\n",
    "weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "\n",
    "# Define dataset\n",
    "dataset = aug_dataset\n",
    "num_classes = 18\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_model_kfold(model_class, weights, dataset, criterion, device, num_epochs=5, n_splits=5)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6257269,
     "sourceId": 10143803,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6298427,
     "sourceId": 10196905,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6308036,
     "sourceId": 10207104,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6344331,
     "sourceId": 10255970,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6356658,
     "sourceId": 10273430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6356659,
     "sourceId": 10273432,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
